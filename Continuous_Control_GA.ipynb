{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "#from ddpg_agent import Agent\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from mqtt_writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name='one_agent_vis/Reacher_Linux/Reacher.x86_64')\n",
    "#env = UnityEnvironment(file_name='20_agents/Reacher_Linux/Reacher.x86_64')\n",
    "#env = UnityEnvironment(file_name='20_agents/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "#env = UnityEnvironment(file_name='one_agent/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='./Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "writer = SummaryWriter(comment=\"-reacher-ga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state_size': 33, 'action_size': 4, 'number_of_agents': 20, 'device': device(type='cpu'), 'hidden_size': 256, 'sigma': 0.05}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \n",
    "        'state_size':  env_info.vector_observations.shape[1],\n",
    "        'action_size': brain.vector_action_space_size,\n",
    "        'number_of_agents': len(env_info.agents),\n",
    "        'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'hidden_size': 256,\n",
    "        'sigma': 0.05,\n",
    "        \n",
    "    \n",
    "}\n",
    "NUM_AGENTS = len(env_info.agents)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set Params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = env_info.vector_observations.shape[1]\n",
    "ACTION_SIZE = brain.vector_action_space_size\n",
    "BRAIN_NAME = env.brain_names[0]\n",
    "NUM_AGENTS = len(env_info.agents)\n",
    "NOISE_STD = 0.01\n",
    "POPULATION_SIZE = 25\n",
    "PARENTS_COUNT = int(POPULATION_SIZE / 5)\n",
    "MAX_STEPS = 1000\n",
    "HIDDEN_LAYER = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, act_size, hid_size=HIDDEN_LAYER):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(obs_size, hid_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hid_size, act_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.netId = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor(x)\n",
    "        return self.mu(x)\n",
    "    \n",
    "    def setNetId(self, netId):\n",
    "        self.netId  = netId\n",
    "\n",
    "    def getNetId(self):\n",
    "        return self.netId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, net):\n",
    "    global step_count, writer\n",
    "    env_info = env.reset(train_mode=True)[BRAIN_NAME]    \n",
    "    states = env_info.vector_observations                 \n",
    "    scores = np.zeros(NUM_AGENTS)\n",
    "    steps = 0\n",
    "    while True:\n",
    "        obs_v = torch.Tensor(states)\n",
    "        #state = torch.from_numpy(states)\n",
    "        actions = net(obs_v)\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[BRAIN_NAME]\n",
    "        steps +=1\n",
    "        next_states = env_info.vector_observations         \n",
    "        rewards = env_info.rewards                         \n",
    "        dones = env_info.local_done                     \n",
    "        scores += env_info.rewards                      \n",
    "        states = next_states\n",
    "        \n",
    "        if steps >= MAX_STEPS or np.any(dones):                                  \n",
    "            break\n",
    "    #print(step_count, \"score\", np.mean(scores))\n",
    "    writer.add_scalar(\"step\", step_count, step_count)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_parent(net):\n",
    "    new_net = copy.deepcopy(net)\n",
    "    for p in new_net.parameters():\n",
    "        noise_t = torch.tensor(np.random.normal(size=p.data.size()).astype(np.float32))\n",
    "        p.data += NOISE_STD * noise_t\n",
    "    return new_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_net = Net(STATE_SIZE, ACTION_SIZE, 128)\n",
    "good_net.load_state_dict(torch.load(\"models/best-ga.pth\"))\n",
    "global step_count\n",
    "step_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.55349985351786"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env, good_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = [\n",
    "    mutate_parent(good_net)\n",
    "    for _ in range(POPULATION_SIZE)\n",
    "]\n",
    "\n",
    "for i in range(len(nets)):\n",
    "    nets[i].setNetId(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = []\n",
    "step_count = 0\n",
    "for net in nets:\n",
    "    step_count +=1\n",
    "    population.append((net, evaluate(env,net)))\n",
    "              \n",
    "    #(net, evaluate(env, net))\n",
    "    #for net in nets\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: reward_mean=6.19, reward_max=6.33, reward_std=0.08\n",
      "1: reward_mean=6.70, reward_max=7.83, reward_std=0.60\n",
      "2: reward_mean=6.80, reward_max=7.83, reward_std=0.70\n"
     ]
    }
   ],
   "source": [
    "writer.reset()\n",
    "timestamp =  int(time.time())\n",
    "log_msg = \"%d New Run  Pop size: %d Noise: %.3f  Parents: %d\" %(timestamp, POPULATION_SIZE, NOISE_STD, PARENTS_COUNT )\n",
    "writer.log(log_msg)\n",
    "gen_idx = 0\n",
    "prev_reward = 0\n",
    "while True:\n",
    "    population.sort(key=lambda p: p[1], reverse=True)\n",
    "    rewards = [p[1] for p in population[:PARENTS_COUNT]]\n",
    "    reward_mean = np.mean(rewards)\n",
    "    reward_max = np.max(rewards)\n",
    "    reward_std = np.std(rewards)\n",
    "\n",
    "    writer.add_scalar(\"episode\", gen_idx, gen_idx)\n",
    "    writer.add_scalar(\"average\", reward_mean, gen_idx)\n",
    "    writer.add_scalar(\"reward_std\", reward_std, gen_idx)\n",
    "    writer.add_scalar(\"score\", reward_max, gen_idx)\n",
    "    msg = \"%d: reward_mean=%.2f, reward_max=%.2f, reward_std=%.2f\" % (\n",
    "        gen_idx, reward_mean, reward_max, reward_std)\n",
    "    writer.log(msg)\n",
    "    print(msg)\n",
    "    if reward_mean > 30:\n",
    "        print(\"Solved in %d steps\" % gen_idx)\n",
    "        break\n",
    "    if reward_max > prev_reward + 1:\n",
    "        #save weights\n",
    "        best_net = population[0][0]\n",
    "        torch.save(best_net.state_dict(), f\"models/ga-{HIDDEN_LAYER}-{int(reward_max)}.pth\")\n",
    "        prev_reward = reward_mean\n",
    "\n",
    "    # generate next population\n",
    "    prev_population = population\n",
    "    population = [population[0]]\n",
    "    for _ in range(POPULATION_SIZE-1):\n",
    "        parent_idx = np.random.randint(0, PARENTS_COUNT)\n",
    "        parent = prev_population[parent_idx][0]\n",
    "        net = mutate_parent(parent)\n",
    "        fitness = evaluate(env, net)\n",
    "        population.append((net, fitness))\n",
    "    gen_idx += 1\n",
    "\n",
    "pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
